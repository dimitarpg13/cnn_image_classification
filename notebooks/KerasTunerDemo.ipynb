{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1a636f-dfa3-4b7a-aade-cd8bc214b498",
   "metadata": {},
   "source": [
    "# KerasTuner demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6062481-060f-42c2-aac4-bd088032b277",
   "metadata": {},
   "source": [
    "`KerasTuner` is a general purpose hyperparameter tuning library. It is integrated with Keras worflows but it is not limited to them. It can be used to tune `scikit-learn` models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503ccd6-f735-4d00-a762-6635e1770aad",
   "metadata": {},
   "source": [
    "## Define the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce4972-4396-47fd-828d-450bbd317cc8",
   "metadata": {},
   "source": [
    "In the following code example, we deinfe a Keras model with two `Dense` layers. We want to tune the number of units in the first `Dense` layer. We just define an integer hyperparameter with `hp.Int('units', min_value=32, max_value=512, step=32)` whose range is from `32` to `512`inclusive. When sampling from it, the minimum step for walking through the interval is `32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859187bb-1adb-4b33-8d31-e05e184f4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "def build_model_with_two_dense_layers(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            # define the hyperparameter\n",
    "            units=hp.Int(\"unit\", min_value=32, max_value=512, step=32),\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f5665-08bd-4b1d-bea7-ab2575ddd241",
   "metadata": {},
   "source": [
    "Let us test if the model was built successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c7f49d-e894-46c5-ad9d-5a36f05a5a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_2, built=False>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner\n",
    "\n",
    "build_model_with_two_dense_layers(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9516d4b-bcda-465f-b406-970edf37a85a",
   "metadata": {},
   "source": [
    "There are many other types of hyperparameters as well. We can define multiple hyperparameters in the function. In the following code, we tune whether to use a `Dropout` layer with `hp.Boolean()`, tine which activation function to use with `hp.Choice`, tune the learning rate of the optimizer with `hp.Float()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5ca3ac0-2e48-4a77-bad1-3683bfa1b834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_3, built=False>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model_with_two_dense_layers_and_dropout(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            # Tune number of units\n",
    "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "            # Tune the activation function to use\n",
    "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        )\n",
    "    )\n",
    "    # Tune whether to use dropout.\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    # Define the optimizer learning rate as a hyperparameter\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "build_model_with_two_dense_layers_and_dropout(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8b989-cb0c-4016-8a48-5a5da6beb8f6",
   "metadata": {},
   "source": [
    "As shown below, the hyperparameters are actual values. In fact, they are just functions returning actual values. For example, `hp.Int()` returns an `int` value. Therefore, you can put them into variables, `for` loops, or `if` conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8eac4e-9998-444c-b30a-ef33beb98ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "print(hp.Int(\"units\", min_value=32, max_value=512, step=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63e315-44a2-455a-8806-da00490ef367",
   "metadata": {},
   "source": [
    "The hyperparameters can be defined in advance and keep the Keras model code in a separate function as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cbec3df-77d2-44a5-bfbf-6a086a2174de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_4, built=False>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_existing_code(units, activation, dropout, lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=units, activation=activation))\n",
    "    if dropout:\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int(\"units\", min_value=32, max_value=512, step=32)\n",
    "    activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    # call existing model-building code with the hyperparameter values\n",
    "    model = call_existing_code(\n",
    "        units=units, activation=activation, dropout=dropout, lr=lr\n",
    "    )\n",
    "    return model\n",
    "\n",
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c4d73-a8ec-4d2a-b569-8f75f31af2cb",
   "metadata": {},
   "source": [
    "Each of the hyperparameters is uniquely identified by its name - the first argument of the corresponding hp type.\n",
    "To tune the number of units in different `Dense` layers separately as different hyperparameters, we give them different names as `f\"units_{i}\"`.\n",
    "\n",
    "Notably, this is also an example of creating conditional hyperparameters. There are many hyperparameters specifying the number of units in the `Dense` layers. The number of such hyperparameters is decided by the number of layers, which is also a hyperparameter.  Therefore, the total number of hyperparameters used may be different from trial to trial. Some hyperparameter is only used when a certain condition is satisfied. For example, `units_3` is only used when `num_layers` is larger than 3. With `KerasTuner`, one can easily define such hyperparameters dynamically while creating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49ca93f5-3835-40b1-a0cb-4a017d854643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_5, built=False>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model_conditional_hpars(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    # tune the number of layers\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # tune number of units separately\n",
    "                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "            \n",
    "build_model_conditional_hpars(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe756d8-6bbe-40ac-a937-f755fb3d3eea",
   "metadata": {},
   "source": [
    "## Start the search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592e265-9e32-41db-85e6-8a3914df5a5b",
   "metadata": {},
   "source": [
    "After defining the search space, we need to select a tuner class to run the search. The available algorithms are `RandomSearch`, `BayesianOptimization`, and `Hyperband`. Here we use `RandomSearch` as an example.\n",
    "\n",
    "To initialize the tuner we need to specify several arguments in the initializer.\n",
    "\n",
    "* `hypermodel`: the model building function which is `build_model` in our case\n",
    "* `objective`: the name of the objective to optimize. Note that whether to minimize or maximize is automatically inferred for built-in metrics.\n",
    "* `max_trials`: the total number of trials to run during the search\n",
    "* `executions_per_trial`: the number of models that should be built and fit for each trial. Different trialshave different hyperparameter values. The executions within the same trial have the same hyperparameter values. The purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. If we want to get results faster we could set `executions_per_trial=1` (single round of training for each model configuration)\n",
    "* `overwrite`: control whether to overwrite the previous results in the same directory or resume the previous search instead. Here we set `overwrite=True` to start a new search and ignore any previous results.\n",
    "* `directory`: a path to a directory for storing the search results.\n",
    "* `project_name`: the name of the sub-directory in the `directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c8b43d-0727-47fe-805b-d22731965e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel=build_model_conditional_hpars,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33dcf3d-716e-47a0-a08e-46b7a8db5ae4",
   "metadata": {},
   "source": [
    "Printing summary of the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa4e3fb2-d7d0-4dbb-88ec-87541ff6d2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 5\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "dropout (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cbab9-565e-4c94-938c-f6a5262e5acd",
   "metadata": {},
   "source": [
    "## Preparation of the MNIST dataset for the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9691101-2c32-4d7c-a3c2-47c0bf9560e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x[:-10000]\n",
    "x_val = x[-10000:]\n",
    "y_train = y[:-10000]\n",
    "y_val = y[-10000:]\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
    "x_val = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
    "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
    "\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72be40-e24b-4a96-9f84-a4ee85708f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
